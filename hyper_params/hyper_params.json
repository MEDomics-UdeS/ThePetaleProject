{
    "lr":{
        "min": 0.001,
        "max": 0.1
    },
    "batch_size":{
        "min": 5,
        "max": 100,
        "step": 5
    },
    "n_layers":{
        "min": 0,
        "max": 1
    },
    "n_units":{
        "min": 1,
        "max": 100
    },
    "dropout":{
        "min":0,
        "max":1
    },
    "activation":{
        "values": ["ReLU","PReLU","LeakyReLU"]
    }

}