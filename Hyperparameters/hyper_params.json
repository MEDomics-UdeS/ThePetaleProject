{
    "lr":{
        "min": 0.0001,
        "max": 0.01
    },
    "batch_size":{
        "min": 5,
        "max": 100,
        "step": 5
    },
    "n_layers":{
        "min": 1,
        "max": 3
    },
    "n_units":{
        "min": 1,
        "max": 50
    },
    "dropout":{
        "min":0,
        "max":1
    },
    "activation":{
        "values": ["ReLU","PReLU","LeakyReLU"]
    },
    "weight_decay": {
        "min": 1e-10,
        "max": 1e-1
    }

}